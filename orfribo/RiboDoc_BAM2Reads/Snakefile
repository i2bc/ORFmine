configfile: "/workdir/config.yaml"


orfribo_version = "0.8.6"


# Imports
from optparse import OptionParser
import gffutils
import re
import os


# Sets the number of threads for multi-threading steps
multi_threads_nbr = 3
mem_mb_resources = 10000


# Sets paths for inside the container
local_path = "/workdir/orfribo/"
database_path = "/database/"
fastq_path = "/fastq/"
ribodoc_tools = "/ORFmine/orfribo/RiboDoc_BAM2Reads/tools/"
snakemake_log_path = "/.snakemake/log/"


# Wildcards definition
SAMPLES, = glob_wildcards(fastq_path + "{sample}.fastq.gz")
BOWTIE2 = ["1","2","3","4","rev.1","rev.2"]
HISAT2 = ["1","2","3","4","5","6","7","8"]
LENGTHS = list(map(str,range(int(config['readsLength_min']),int(config['readsLength_max'])+1)))


# Strings with minimum and maximum read lengths to be used in file names
frag_length_S = "." + LENGTHS[0]
frag_length_L = "." + LENGTHS[0] + "-" + LENGTHS[len(LENGTHS)-1]


# Check if attributes specified by the user are present in the gff file
f = open(database_path + config['gff'],"r")
name_in_gff = False
for l in f:
    is_name = re.search("^([^\t]+[\t]){8}.*" + config['gff_name_attribut'],l)
    if is_name:
        name_in_gff = True
f.close()


rule all:
    input:
        # Call for fastqc quality control rule
        expand(local_path + "RESULTS/fastqc/{sample}_fastqc.html",sample=SAMPLES),
        # Alignment statistics report
        local_path + "RESULTS/" + config['project_name'] + ".Analysis_Report.txt",
        # Call for count matrix creations on all ORFs
        local_path + "RESULTS/Bam2Reads_genome_output/all_samples_genome" + frag_length_L + ".mean" + config['orfstats_mean_threshold'] + "_median" + config['orfstats_median_threshold'] + "_reads_concatenated.tab"


# When RiboDoc starts
onstart:
    shell("mkdir -p " + local_path + "logs/;")
    shell("echo 'RiboDoc version : " + orfribo_version + "' > " + local_path + "logs/RiboDoc_package_versions.txt;")
    shell("conda list >> " + local_path + "logs/RiboDoc_package_versions.txt;")

# When the jobs are all done
onsuccess:
    # Copy config file to keep trace of parameters used
    shell("cp /workdir/config.yaml " + local_path + "RESULTS/ ;")

# If anything goes wrong
onerror:
    # Copy config file to keep trace of parameters used
    shell("cp /workdir/config.yaml " + local_path + "RESULTS/ ;")


# Find the adapter sequence if not set in config file
rule find_adapter_sequence:
    input:
        fastq = fastq_path + "{sample}.fastq.gz"
    output:
        adapter = local_path + "RESULTS/adapter_lists/{sample}.txt"
    log:
        rscript = local_path + "logs/find_adapter_sequence/{sample}.rscript.log",
        sed = local_path + "logs/find_adapter_sequence/{sample}.sed.log",
        echo = local_path + "logs/find_adapter_sequence/{sample}.echo.log",
        touch = local_path + "logs/find_adapter_sequence/{sample}.touch.log"
    shell:
        "touch {output.adapter} 2> {log.touch};"
        "if [ -z " + config['adapt_sequence'] + " ]; then "
        "Rscript " + ribodoc_tools + "find_adapter_sequence.R {input.fastq} 2> {log.rscript} ;"
        "elif [ '" + config['already_trimmed'] + "' = 'no' ]; then echo " + config['adapt_sequence'] + " 1> {output.adapter} 2> {log.echo};"
        "fi;"

# Adds transcript names and gene IDs to the CDS and exon lines if possible
rule name_CDS:
    input:
        gff = database_path + os.path.basename(config['gff'])
    output:
        gff_namedCDS = local_path + "RESULTS/annex_database/NamedCDS_" + os.path.basename(config['gff'])
    run:
        gene_id_bool = True
        if name_in_gff == True:
            db = gffutils.create_db(input.gff, ':memory:', merge_strategy='create_unique', keep_order=True)
            with open(output.gff_namedCDS, 'w') as fout:
                for d in db.directives:
                    fout.write('##{0}\n'.format(d))
                for feature in db.all_features():
                    if feature.featuretype == config['gff_cds_feature'] or feature.featuretype == "exon":
                        parent = list(db.parents(feature, level=1))
                        if len(parent) > 0:
                            parent = parent[0]
                            if parent.attributes.get(config['gff_name_attribut']) and not feature.attributes.get(config['gff_name_attribut']):
                                feature.attributes[config['gff_name_attribut']] = [i.replace("mRNA","cds") for i in parent.attributes.get(config['gff_name_attribut'])]
                                feature.attributes[config['gff_name_attribut']][0] + "_name"
                            if parent.attributes.get('ID') and not feature.attributes.get('ID'):
                                feature.attributes["ID"] = parent.attributes["ID"]
                                feature.attributes['ID'] = feature.attributes['ID'][0] + "_CDS"
                    fout.write(str(feature) + '\n')
        else:
            shell("cp {input.gff} {output.gff_namedCDS} ;")
        if gene_id_bool:
            print("'gene_id' attributes are present.")
        else:
            print("Missing at least some 'gene_id' attribute in this gff.")
        shell("sed -i -E 's/\\s/\\t/8' {output.gff_namedCDS} ;")

# Quality control of data : build of the fastqc
rule make_fastqc:
    input:
        fastq_path + "{sample}.fastq.gz"
    output:
        local_path + "RESULTS/fastqc/{sample}_fastqc.zip",
        local_path + "RESULTS/fastqc/{sample}_fastqc.html"
    log:
        local_path + "logs/make_fastqc/{sample}.log"
    benchmark:
        local_path + "benchmarks/make_fastqc/{sample}.benchmark.txt"
    params:
       outdir = local_path + "RESULTS/fastqc/"
    shell:
        "fastqc {input} --outdir {params.outdir} 2> {log} ;"

# Removes/cuts potential adapters on the reads
rule adapt_trimming:
    input:
        fastq = fastq_path + "{sample}.fastq.gz",
        adapt_seq = local_path + "RESULTS/adapter_lists/{sample}.txt"
    output:
        cut_fastq = temp(local_path + "RESULTS/cutadapt/{sample}.cutadapt" + frag_length_L + ".fastq.gz")
    log:
        trim_value = local_path + "logs/adapt_trimming/{sample}_trim_value.log",
        cutadapt = local_path + "logs/adapt_trimming/{sample}_cutadapt.log",
        cutadapt_out = local_path + "logsTmp/{sample}_adapt_trimming.log"
    benchmark:
        local_path + "benchmarks/adapt_trimming/{sample}.benchmark.txt"
    resources:
        mem_mb = mem_mb_resources
    threads:
        multi_threads_nbr
    shell:
        "adapter_sequence=`cat {input.adapt_seq}` ;"
        "if [ '" + config['already_trimmed'] + "' = 'no' ]; then trim=\"-a ${{adapter_sequence}} --trimmed-only\"; else trim=''; fi 2> {log.trim_value};"
        "cutadapt ${{trim}} -e 0.125 -j {threads} --max-n=1 -m " + config['readsLength_min'] + " -M " + config['readsLength_max'] + " -o {output.cut_fastq} {input.fastq} 1>> {log.cutadapt_out} 2> {log.cutadapt} ;"

# Builds the index of bowtie2 mapping on sequences for reads remove
rule bowtie2_build_outRNA:
    input:
        outRNA = database_path + os.path.basename(config['fasta_outRNA'])
    output:
        expand(local_path + "RESULTS/annex_database/outRNA_bowtie2.{extb}.bt2",extb=BOWTIE2)
    params:
        outNames = local_path + "RESULTS/annex_database/outRNA_bowtie2"
    log:
        local_path + "logs/bowtie2_build_outRNA/bowtie2_build_outRNA.log"
    benchmark:
        local_path + "benchmarks/bowtie2_build_outRNA/bowtie2_build_outRNA.benchmark.txt"
    threads:
        multi_threads_nbr
    shell:
        "bowtie2-build --threads {threads} {input.outRNA} {params.outNames} &> {log} ;"

# Mapping of non-coding RNA
rule bowtie2_run_outRNA:
    input:
        expand(local_path + "RESULTS/annex_database/outRNA_bowtie2.{extb}.bt2",extb=BOWTIE2),
        fastq = local_path + "RESULTS/cutadapt/{sample}.cutadapt" + frag_length_L + ".fastq.gz"
    output:
        local_path + "RESULTS/no-outRNA/{sample}" + frag_length_L + ".no-outRNA.fastq.gz"
    log:
        bt2 = local_path + "logsTmp/{sample}_bowtie2_run_outRNA.log"
    benchmark:
        local_path + "benchmarks/bowtie2_run_outRNA/{sample}.benchmark.txt"
    resources:
        mem_mb = mem_mb_resources
    params:
        sample_names = "{sample}"
    threads:
        multi_threads_nbr
    shell:
        "bowtie2 -x " + local_path + "RESULTS/annex_database/outRNA_bowtie2 --threads {threads} -U {input.fastq} --un-gz {output} > /dev/null 2>> {log.bt2} ;"

# Builds the index of bowtie2 mapping for all RNA
rule bowtie2_build:
    input:
        fasta = database_path + os.path.basename(config['fasta'])
    output:
        expand(local_path + "RESULTS/annex_database/index_bowtie2.{extb}.bt2",extb=BOWTIE2)
    log:
        local_path + "logs/bowtie2_build/bowtie2_build.log"
    benchmark:
        local_path + "benchmarks/bowtie2_build/bowtie2_build.benchmark.txt"
    params:
        outNames = local_path + "RESULTS/annex_database/index_bowtie2"
    threads:
        multi_threads_nbr
    shell:
        "bowtie2-build --threads {threads} {input.fasta} {params.outNames} &> {log} ;"

# Builds the index of hisat2 mapping for all RNA
rule hisat2_build:
    input:
        fasta = database_path + os.path.basename(config['fasta'])
    output:
        expand(local_path + "RESULTS/annex_database/index_hisat2.{exth}.ht2",exth=HISAT2)
    log:
        local_path + "logs/hisat2_build/hisat2_build.log"
    benchmark:
        local_path + "benchmarks/hisat2_build/hisat2_build.benchmark.txt"
    params:
        outNames = local_path + "RESULTS/annex_database/index_hisat2"
    threads:
        multi_threads_nbr
    shell:
        "hisat2-build --threads {threads} {input.fasta} {params.outNames} &> {log} ;"

# Mapping of all RNA by bowtie2 and hisat2
rule run_mapping:
    input:
        expand(local_path + "RESULTS/annex_database/index_hisat2.{exth}.ht2",exth=HISAT2),
        expand(local_path + "RESULTS/annex_database/index_bowtie2.{extb}.bt2",extb=BOWTIE2),
        fastq = local_path + "RESULTS/no-outRNA/{sample}" + frag_length_L + ".no-outRNA.fastq.gz"
    output:
        fastq = temp(local_path + "RESULTS/no-outRNA/{sample}" + frag_length_L + ".no-outRNA.notAlign.fastq.gz"),
        sam_hisat2 = temp(local_path + "RESULTS/BAM/{sample}" + frag_length_L + ".hisat2.sam"),
        sam_bowtie2 = temp(local_path + "RESULTS/BAM/{sample}" + frag_length_L + ".bowtie2.sam")
    log:
        hisat2_out = local_path + "logsTmp/{sample}_run_mapping_hisat2.log",
        bowtie2_out = local_path + "logsTmp/{sample}_run_mapping_bowtie2.log"
    benchmark:
        local_path + "benchmarks/run_mapping/{sample}.benchmark.txt"
    resources:
        mem_mb = mem_mb_resources
    params:
        index_names_hisat2 = local_path + "RESULTS/annex_database/index_hisat2",
        index_names_bowtie2 = local_path + "RESULTS/annex_database/index_bowtie2",
        sample_names="{sample}"
    threads:
        multi_threads_nbr
    shell:
        "hisat2 -x {params.index_names_hisat2} --threads {threads} -U {input.fastq} --un-gz {output.fastq} -S {output.sam_hisat2} 2>> {log.hisat2_out} ;"
        "bowtie2 -x {params.index_names_bowtie2} --threads {threads} -U {output.fastq} -S {output.sam_bowtie2} 2>> {log.bowtie2_out} ;"

# Creates bam and sam files
rule samtools_filter:
    input:
        sam_hisat2 = local_path + "RESULTS/BAM/{sample}" + frag_length_L + ".hisat2.sam",
        sam_bowtie2 = local_path + "RESULTS/BAM/{sample}" + frag_length_L + ".bowtie2.sam"
    output:
        bam = local_path + "RESULTS/BAM/{sample}" + frag_length_L + ".bam"
    log:
        grep_header_hisat2 = local_path + "logs/samtools_filter/{sample}.grep_hisat2.log",
        grep_core_hisat2 =  local_path + "logs/samtools_filter/{sample}.grep_core_hisat2.log",
        ZS_filter_hisat2 = local_path + "logs/samtools_filter/{sample}.NH_filter_hisat2.log",
        XM_filter_hisat2 = local_path + "logs/samtools_filter/{sample}.XM_filter_hisat2.log",
        grep_core_bowtie2 =  local_path + "logs/samtools_filter/{sample}.grep_core_bowtie2.log",
        XS_filter_bowtie2 = local_path + "logs/samtools_filter/{sample}.XS_filter_bowtie2.log",
        XM_filter_bowtie2 = local_path + "logs/samtools_filter/{sample}.XM_filter_bowtie2.log",
        view_bam = local_path + "logs/samtools_filter/{sample}.view_bam.log",
        sort_bam = local_path + "logs/samtools_filter/{sample}.sort_bam.log",
        rm = local_path + "logs/samtools_filter/{sample}.rm.log"
    benchmark:
        local_path + "benchmarks/samtools_filter/{sample}.benchmark.txt"
    resources:
        mem_mb = round(mem_mb_resources / 3)
    params:
        sam = local_path + "RESULTS/BAM/{sample}" + frag_length_L + ".sam"
    threads:
        multi_threads_nbr
    shell:
        "set +o pipefail ;"
        "grep '^@' {input.sam_hisat2}  2> {log.grep_header_hisat2} 1> {params.sam} ;"
        "grep -v '^@' {input.sam_hisat2} 2> {log.grep_core_hisat2} | grep -v 'ZS:i:' 2> {log.ZS_filter_hisat2} | egrep -i 'XM:i:0|XM:i:1' 2> {log.XM_filter_hisat2} 1>> {params.sam} ;"
        "grep -v '^@' {input.sam_bowtie2} 2> {log.grep_core_bowtie2} | grep -v 'XS:i:' 2> {log.XS_filter_bowtie2} | egrep -i 'XM:i:0|XM:i:1' 2> {log.XM_filter_bowtie2} 1>> {params.sam} ;"
        "samtools view -@ {threads} -F 3844 -q 1 -h -b {params.sam} 2> {log.view_bam} | samtools sort -@ {threads} -o {output.bam} 2> {log.sort_bam} ;"
        "rm -f {params.sam} 2> {log.rm};"

# Index BAMs
rule index_bam:
    input:
        bam = local_path + "RESULTS/BAM/{sample}" + frag_length_L + ".bam"
    output:
        bai = local_path + "RESULTS/BAM/{sample}" + frag_length_L + ".bam.bai"
    log:
        index_bam = local_path + "logs/index_bam/{sample}.index_bam.log"
    benchmark:
        local_path + "benchmarks/index_bam/{sample}.benchmark.txt"
    shell:
        "samtools index {input.bam} 2> {log.index_bam} ;"

# Creates an analysis report with trimming and alignment statistics
rule stats_report:
    input:
        ready = expand(rules.samtools_filter.output, sample=SAMPLES)
    output:
        stat_report = local_path + "RESULTS/" + config['project_name'] + ".Analysis_Report.txt"
    run:
        # List of interesting logs to make the report
        logs_names = ["adapt_trimming","bowtie2_run_outRNA","run_mapping_hisat2","run_mapping_bowtie2"]
        # File for the statistical report
        data_report = open(output.stat_report,"w")
        for sample in SAMPLES:
            # Data treatment report creation
            data_report.write("##################\n## NEXT SAMPLE ##\n##################\n\n" + sample + "\n")
            for log in logs_names:
                data_report.write("\n" + ("#" * (len(log)+6)) + "\n## " + log + " ##\n" + ("#" * (len(log)+6)) + "\n")
                logs_files = open(local_path + "logsTmp/" + sample + "_" + log + ".log","r")
                # Keep only lines of interest from cutadapt report1,2
                i=-1
                if log=="adapt_trimming":
                    if int(2) > 1:
                        lines_to_read = range(22)
                    else:
                        lines_to_read = range(20)
                    for position, line in enumerate(logs_files):
                        if position in lines_to_read:
                            data_report.write(line)
                        else:
                            break
                else:
                    for line in logs_files:
                        data_report.write(line)
                logs_files.close()
            data_report.write("\n\n\n")
        data_report.close()

# In case there are no UTRs in the original GFF, call for ORFelongate functions
rule ORFget:
    input:
        fasta = database_path + os.path.basename(config['fasta']),
        gff = local_path + "RESULTS/annex_database/NamedCDS_" + os.path.basename(config['gff'])
    output:
        fasta = local_path + "RESULTS/ORFribo/database/exome_elongated.nfasta",
        gff = local_path + "RESULTS/ORFribo/database/exome_elongated.gff",
        gff_with_genes = local_path + "RESULTS/ORFribo/database/exome_elongated_with_gene_features.gff"
    log:
        orf_get = local_path + "logs/ORFget/orf_get.log"
    benchmark:
        local_path + "benchmarks/ORFget/ORFget.benchmark.txt"
    params:
        path = local_path + "RESULTS/ORFribo/database/exome"
    shell:
        "python3 " + ribodoc_tools + "Bam2Reads_function/ORFget.py -fna {input.fasta} -gff {input.gff} -features_include " + config['gff_cds_feature'] + " -name_attribute " + config['gff_name_attribut'] + " -o {params.path} -type nucl -elongate 50 -check ;"
        "mv {output.gff} {output.gff_with_genes};"
        "awk '$3 !~ /gene/' {output.gff_with_genes} > {output.gff};"

# Create GTF file for riboWaltz
rule exome_construction_gtf:
    input:
        fasta = local_path + "RESULTS/ORFribo/database/exome_elongated.nfasta",
        gff_with_genes = local_path + "RESULTS/ORFribo/database/exome_elongated_with_gene_features.gff"
    output:
        fasta = local_path + "RESULTS/ORFribo/annex_database/exome_elongated.exons_" + os.path.basename(config['fasta']),
        gtf = local_path + "RESULTS/ORFribo/annex_database/exome_elongated.exons_" + os.path.basename(config['gff']) + ".gtf"
    log:
        samtools_index = local_path + "logs/exome_construction_gtf/samtools_index.log",
        gffread_gtf = local_path + "logs/exome_construction_gtf/gffread.log",
        sed = local_path + "logs/exome_construction_gtf/sed.log",
        awk = local_path + "logs/exome_construction_gtf/awk.log"
    benchmark:
        local_path + "benchmarks/exome_construction_gtf/exome_construction_gtf.benchmark.txt"
    params:
        tmp_gtf = local_path + "RESULTS/ORFribo/annex_database/exome_elongated.exons_tmp.gtf"
    shell:
        "samtools faidx {input.fasta} 2> {log.samtools_index} ;"
        "gffread -F -T -w {output.fasta} -o {params.tmp_gtf} -g {input.fasta} {input.gff_with_genes} 2> {log.gffread_gtf} ;"
        "sed -i 's/description[^\;]*\;//' {params.tmp_gtf} 2>> {log.sed} ;"
        "sed -i 's/\\t[A-Z]*[_]*gene_segment\\t/\\ttranscript\\t/' {params.tmp_gtf} 2>> {log.sed} ;"
        """awk -F '\\t' '{{if(NF<=9) {{print($0);}} else {{for(field=1;field<9;field++) {{printf("%s\\t",$field);}} for(field=9;field<=NF;field++) {{printf("%s ",$field);}} printf("\\n");}}}}' {params.tmp_gtf} > {output.gtf} 2>> {log.awk} ;"""
        "rm -f {params.tmp_gtf};"
        "sed -i 's/\\s$//' {output.gtf} 2>> {log.sed} ;"

# Builds the index of bowtie2 mapping for all RNA
rule bowtie2_build_exome:
    input:
        fasta = local_path + "RESULTS/ORFribo/database/exome_elongated.nfasta"
    output:
        expand(local_path + "RESULTS/ORFribo/annex_database/exome_elongated.exome_index_bowtie2.{extb}.bt2",extb=BOWTIE2)
    log:
        local_path + "logs/bowtie2_build_exome/bowtie2_build.log"
    benchmark:
        local_path + "benchmarks/bowtie2_build_exome/exome_index_bowtie2.benchmark.txt"
    params:
        index_names = local_path + "RESULTS/ORFribo/annex_database/exome_elongated.exome_index_bowtie2"
    threads:
        multi_threads_nbr
    shell:
        "bowtie2-build --threads {threads} {input.fasta} {params.index_names} &> {log} ;"

# Builds the index of hisat2 mapping for all RNA
rule hisat2_build_exome:
    input:
        fasta = local_path + "RESULTS/ORFribo/database/exome_elongated.nfasta"
    output:
        expand(local_path + "RESULTS/ORFribo/annex_database/exome_elongated.exome_index_hisat2.{exth}.ht2",exth=HISAT2)
    log:
        local_path + "logs/hisat2_build_exome/hisat2_build.log"
    benchmark:
        local_path + "benchmarks/hisat2_build_exome/hisat2_build_exome.benchmark.txt"
    params:
        index_names = local_path + "RESULTS/ORFribo/annex_database/exome_elongated.exome_index_hisat2"
    threads:
        multi_threads_nbr
    shell:
        "hisat2-build --threads {threads} {input.fasta} {params.index_names} &> {log} ;"

# Performs mapping on exome
rule run_mapping_exome:
    input:
        expand(local_path + "RESULTS/ORFribo/annex_database/exome_elongated.exome_index_hisat2.{exth}.ht2",exth=HISAT2),
        expand(local_path + "RESULTS/ORFribo/annex_database/exome_elongated.exome_index_bowtie2.{extb}.bt2",extb=BOWTIE2),
        fastq = local_path + "RESULTS/no-outRNA/{sample}" + frag_length_L + ".no-outRNA.fastq.gz"
    output:
        sam_hisat2 = temp(local_path + "RESULTS/ORFribo/BAM_exome/{sample}" + frag_length_L + ".hisat2.sam"),
        sam_bowtie2 = temp(local_path + "RESULTS/ORFribo/BAM_exome/{sample}" + frag_length_L + ".bowtie2.sam"),
        fastq = temp(local_path + "RESULTS/no-outRNA/{sample}" + frag_length_L + ".no-outRNA.notAlign.exome.fastq.gz")
    log:
        hisat2_out = local_path + "logs/run_mapping_exome/{sample}_run_mapping_exome_hisat2.log",
        bowtie2_out = local_path + "logs/run_mapping_exome/{sample}_run_mapping_exome_bowtie2.log"
    benchmark:
        local_path + "benchmarks/run_mapping_exome/{sample}.benchmark.txt"
    resources:
        mem_mb = mem_mb_resources
    params:
        index_names_hisat2 = local_path + "RESULTS/ORFribo/annex_database/exome_elongated.exome_index_hisat2",
        index_names_bowtie2 = local_path + "RESULTS/ORFribo/annex_database/exome_elongated.exome_index_bowtie2",
        sample_names = "{sample}"
    threads:
        multi_threads_nbr
    shell:
        "hisat2 -x {params.index_names_hisat2} --threads {threads} -k 10 -U {input.fastq} --un-gz {output.fastq} -S {output.sam_hisat2} 2>> {log.hisat2_out} ;"
        "bowtie2 -x {params.index_names_bowtie2} --threads {threads} -k 10 -U {output.fastq} -S {output.sam_bowtie2} 2>> {log.bowtie2_out} ;"

# Creates bam and sam files
rule samtools_filter_exome:
    input:
        sam_hisat2 = local_path + "RESULTS/ORFribo/BAM_exome/{sample}" + frag_length_L + ".hisat2.sam",
        sam_bowtie2 = local_path + "RESULTS/ORFribo/BAM_exome/{sample}" + frag_length_L + ".bowtie2.sam"
    output:
        bam = local_path + "RESULTS/ORFribo/BAM_exome/exome_elongated.{sample}" + frag_length_L + ".bam"
    log:
        grep_header_hisat2 = local_path + "logs/samtools_filter_exome/{sample}.grep_hisat2.log",
        uniq_header = local_path + "logs/samtools_filter_exome/{sample}.uniq_header.log",
        grep_core_hisat2 =  local_path + "logs/samtools_filter_exome/{sample}.grep_core_hisat2.log",
        XM_filter_hisat2 = local_path + "logs/samtools_filter_exome/{sample}.XM_filter_hisat2.log",
        grep_core_bowtie2 =  local_path + "logs/samtools_filter_exome/{sample}.grep_core_bowtie2.log",
        view_bam = local_path + "logs/samtools_filter_exome/{sample}.view_bam.log",
        sort_bam = local_path + "logs/samtools_filter_exome/{sample}.sort_bam.log",
        rm = local_path + "logs/samtools_filter_exome/{sample}.rm.log"
    benchmark:
        local_path + "benchmarks/samtools_filter_exome/{sample}.benchmark.txt"
    resources:
        mem_mb = round(mem_mb_resources / 3)
    params:
        sample = "{sample}",
        sam = local_path + "RESULTS/ORFribo/BAM_exome/{sample}" + frag_length_L + ".sam"
    threads:
        multi_threads_nbr
    shell:
        "set +o pipefail ;"
        "grep '^@' {input.sam_hisat2}  2> {log.grep_header_hisat2} | uniq 2> {log.uniq_header} 1> {params.sam} ;"
        "grep -v '^@' {input.sam_hisat2} 2> {log.grep_core_hisat2} | egrep -i 'XM:i:0|XM:i:1' 2> {log.XM_filter_hisat2} 1>> {params.sam} ;"
        "grep -v '^@' {input.sam_bowtie2} 2> {log.grep_core_bowtie2} 1>> {params.sam} ;"
        "samtools view -@ {threads} -F 3588 -h -b {params.sam} 2> {log.view_bam} | samtools sort -@ {threads} -o {output.bam} 2> {log.sort_bam} ;"
        "rm -f {params.sam}  2> {log.rm};"

# Index BAMs
rule index_bam_exome:
    input:
        bam = local_path + "RESULTS/ORFribo/BAM_exome/exome_elongated.{sample}" + frag_length_L + ".bam"
    output:
        bai = local_path + "RESULTS/ORFribo/BAM_exome/exome_elongated.{sample}" + frag_length_L + ".bam.bai"
    log:
        index_bam = local_path + "logs/index_bam_exome/{sample}.index_bam.log"
    benchmark:
        local_path + "benchmarks/index_bam_exome/{sample}.benchmark.txt"
    shell:
        "samtools index {input.bam} 2> {log.index_bam} ;"

# Performs qualitative analysis with riboWaltz
rule riboWaltz_exome:
    input:
        exome_gtf = local_path + "RESULTS/ORFribo/annex_database/exome_elongated.exons_" + os.path.basename(config['gff']) + ".gtf",
        exome_bam = expand(rules.samtools_filter_exome.output, sample=SAMPLES)
    output:
        psite_table = local_path + "RESULTS/ORFribo/riboWaltz/psite_offset.csv"
    log:
        periodicity = local_path + "logs/riboWaltz_exome/riboWaltz.log"
    resources:
        mem_mb = mem_mb_resources
    benchmark:
        local_path + "benchmarks/riboWaltz_exome/riboWaltz.benchmark.txt"
    shell:
        "touch {output.psite_table};"
        "Rscript " + ribodoc_tools + "periodicity_riboWaltz_exome.R {input.exome_gtf} 2> {log} ;"
        "rm -f " + local_path + "Rplots.pdf ;"

# Calls Bam2Reads functions
rule Bam2Reads_exome:
    input:
        gff = local_path + "RESULTS/ORFribo/database/exome_elongated.gff",
        bam = local_path + "RESULTS/ORFribo/BAM_exome/exome_elongated.{sample}" + frag_length_L + ".bam",
        bai = local_path + "RESULTS/ORFribo/BAM_exome/exome_elongated.{sample}" + frag_length_L + ".bam.bai",
        psite_table = local_path + "RESULTS/ORFribo/riboWaltz/psite_offset.csv"
    output:
        reads = local_path + "RESULTS/ORFribo/Bam2Reads_exome_output/{sample}_{length}/exome" + frag_length_L + "_reads.tab"
    log:
        bam2read = local_path + "logs/Bam2Reads_exome/{sample}.{length}.bam2read.log",
        offset_grep = local_path + "logs/Bam2Reads_exome/{sample}.{length}.offset_grep.log"
    benchmark:
        local_path + "benchmarks/Bam2Reads_exome/{sample}.{length}.bam2read.benchmark.txt"
    resources:
        mem_mb = round(mem_mb_resources / 2)
    params:
        outdir = local_path + "RESULTS/ORFribo/Bam2Reads_exome_output/{sample}_{length}/",
        sample_name = "{sample}",
        read_length = "{length}"
    shell:
        "set +o pipefail;"
        "offset=$(grep {params.sample_name} {input.psite_table} 2> {log.offset_grep} | grep ^{params.read_length} 2>> {log.offset_grep} | cut -f7 2>> {log.offset_grep}) 2>> {log.offset_grep};"
        "if [ $offset = '' ]; then offset=12; fi;"
        "python3 " + ribodoc_tools + "Bam2Reads_function/BAM2Reads.py -shift ${{offset}} -kmer {params.read_length} -gff {input.gff} -bam {input.bam} -outpath {params.outdir} -outname exome" + frag_length_L + " -features_include " + config['gff_cds_feature'] + " 2> {log.bam2read};"
        ""

# Calls ORFstat functions
rule ORFstats:
    input:
        psite_table = local_path + "RESULTS/ORFribo/riboWaltz/psite_offset.csv",
        reads = local_path + "RESULTS/ORFribo/Bam2Reads_exome_output/{sample}_{length}/exome" + frag_length_L + "_reads.tab"
    output:
        stats = local_path + "RESULTS/ORFribo/Bam2Reads_exome_output/{sample}_{length}/exome" + frag_length_L + "_reads.stats"
    log:
        orfstats = local_path + "logs/ORFstats/{sample}.{length}.orfstats.log"
    benchmark:
        local_path + "benchmarks/ORFstats/{sample}.{length}.orfstats.benchmark.txt"
    params:
        sample_name = "{sample}",
        read_length = "{length}"
    shell:
        "set +o pipefail;"
        "python3 " + ribodoc_tools + "Bam2Reads_function/ORFstats.py -tab {input.reads} -N 10 2> {log.orfstats};"

# Select reads of specific length (determined by ORFstats)
rule select_read_lengths:
    input:
        orfstats = expand(local_path + "RESULTS/ORFribo/Bam2Reads_exome_output/{sample}_{length}/exome" + frag_length_L + "_reads.stats", sample=SAMPLES, length=LENGTHS)
    output:
        table = local_path + "RESULTS/selected_length_tables/threshold_mean" + config['orfstats_mean_threshold'] + "_median" + config['orfstats_median_threshold'] + "/{sample}.txt"
    benchmark:
        local_path + "benchmarks/select_read_lengths/{sample}.select_read_lengths.benchmark_mean" + config['orfstats_mean_threshold'] + "_median" + config['orfstats_median_threshold'] + ".txt"
    params:
        orfstats_path = local_path + "RESULTS/ORFribo/Bam2Reads_exome_output/{sample}_",
        orfstats_name = "/exome" + frag_length_L + "_reads.stats"
    threads:
        multi_threads_nbr
    run:
        pattern_mean = re.compile("^Mean[\s\t]+([\w.]+)")
        pattern_median = re.compile("^Median[\s\t]+([\w.]+)")
        file_table = open(output.table, "x")
        for length in LENGTHS:
            mean_med = 0
            file_gff = open(params.orfstats_path + length + params.orfstats_name, "r")
            for line in file_gff:
                match_mean = pattern_mean.match(line)
                match_median = pattern_median.match(line)
                if bool(match_mean):
                    if float(match_mean.group(1)) >= float(config['orfstats_mean_threshold']):
                        mean_med = 1
                if bool(match_median):
                    if float(match_median.group(1)) >= float(config['orfstats_median_threshold']):
                        mean_med += 1
            if mean_med == 2:
                file_table = open(output.table, "a")
                file_table.write(length + "\n")
                file_table.close()
            file_gff.close()

# Copies the output of ORFtrack into the 'database' directory if it is not already in it
rule ORFtrack_output_copy:
    input:
        intergenic_gff = "/workdir/mapping_orf_" + os.path.splitext(os.path.basename(config['fasta']))[0] + ".gff"
    output:
        intergenic_gff = database_path + "mapping_orf_" + os.path.splitext(os.path.basename(config['fasta']))[0] + ".gff"
    shell:
        "cp {input.intergenic_gff} {output.intergenic_gff};"

# Calls Bam2Reads functions for the genome steps
rule Bam2Reads_genome:
    input:
        intergenic_gff = database_path + os.path.basename(config['gff_intergenic']),
        bam = local_path + "RESULTS/BAM/{sample}" + frag_length_L + ".bam",
        bai = local_path + "RESULTS/BAM/{sample}" + frag_length_L + ".bam.bai",
        psite_table = local_path + "RESULTS/ORFribo/riboWaltz/psite_offset.csv",
        table = local_path + "RESULTS/selected_length_tables/threshold_mean" + config['orfstats_mean_threshold'] + "_median" + config['orfstats_median_threshold'] + "/{sample}.txt"
    output:
        reads_table = local_path + "RESULTS/Bam2Reads_genome_output/{sample}/length_{length}/genome" + frag_length_L + ".mean" + config['orfstats_mean_threshold'] + "_median" + config['orfstats_median_threshold'] + "_reads.tab"
    log:
        bam2read = local_path + "logs/Bam2Reads_genome/{sample}.{length}.bam2read_mean" + config['orfstats_mean_threshold'] + "_median" + config['orfstats_median_threshold'] + ".log",
        offset_grep = local_path + "logs/Bam2Reads_genome/{sample}.{length}.offset_grep_mean" + config['orfstats_mean_threshold'] + "_median" + config['orfstats_median_threshold'] + ".log"
    benchmark:
        local_path + "benchmarks/Bam2Reads_genome/{sample}.{length}.Bam2Reads_genome.benchmark._mean" + config['orfstats_mean_threshold'] + "_median" + config['orfstats_median_threshold'] + "txt"
    resources:
        mem_mb = mem_mb_resources
    params:
        outdir = local_path + "RESULTS/Bam2Reads_genome_output/{sample}/length_{length}/",
        sample_name = "{sample}",
        reads_length = "{length}"
    run:
        in_lengths_list = ""
        lengths_list = open(input.table,"r").readlines()
        for length in lengths_list:
            newlength = length.rstrip('\r\n')
            if newlength == params.reads_length:
                shell("""
                offset=$(grep '{params.sample_name}' {input.psite_table} 2> {log.offset_grep} | grep '^{params.reads_length}' 2>> {log.offset_grep} | cut -f7 2>> {log.offset_grep}) 2>> {log.offset_grep};
                python3 """ + ribodoc_tools + "Bam2Reads_function/BAM2Reads.py -shift ${{offset}} -kmer {params.reads_length} -gff {input.intergenic_gff} -bam {input.bam} -outpath {params.outdir} -outname genome" + frag_length_L + ".mean" + config['orfstats_mean_threshold'] + "_median" + config['orfstats_median_threshold'] + " -features_include " + config['final_counts'] + " 2> {log.bam2read};")
                in_lengths_list = True
                break
        if not in_lengths_list:
            fake = open(output.reads_table,"x")


# Concatenate counts tables selected into one by sample
rule concatenate_count_tables_genome:
    input:
        tables_list = expand(rules.Bam2Reads_genome.output, sample=SAMPLES, length=LENGTHS)
    output:
        table = local_path + "RESULTS/Bam2Reads_genome_output/{sample}/genome" + frag_length_L + ".mean" + config['orfstats_mean_threshold'] + "_median" + config['orfstats_median_threshold'] + "_reads_concatenated.tab"
    log:
        local_path + "logs/concatenate_count_tables_genome/{sample}.concatenate_mean" + config['orfstats_mean_threshold'] + "_median" + config['orfstats_median_threshold'] + ".log"
    benchmark:
        local_path + "benchmarks/concatenate_count_tables_genome/{sample}.concatenate.benchmark_mean" + config['orfstats_mean_threshold'] + "_median" + config['orfstats_median_threshold'] + ".txt"
    params:
        outdir = local_path + "RESULTS/Bam2Reads_genome_output/{sample}/",
        sample_name = "{sample}"
    run:
        selected_tables = []
        for table in input.tables_list:
            if os.stat(table).st_size > 0:
                if params.sample_name in str(table):
                    selected_tables.append(table)
        table_string = ' '.join(selected_tables)
        print(table_string)
        shell("python3 " + ribodoc_tools + "Bam2Reads_function/concatenate.py -tables " + table_string + " -outpath {params.outdir} -outname genome" + frag_length_L + ".mean" + config['orfstats_mean_threshold'] + "_median" + config['orfstats_median_threshold'])

# Concatenate all samples concatenated tables together :
rule concatenate_all_tables:
    input:
        tables_list = expand(rules.concatenate_count_tables_genome.output, sample=SAMPLES)
    output:
        table = local_path + "RESULTS/Bam2Reads_genome_output/all_samples_genome" + frag_length_L + ".mean" + config['orfstats_mean_threshold'] + "_median" + config['orfstats_median_threshold'] + "_reads_concatenated.tab"
    log:
        local_path + "logs/concatenate_all_tables/concatenate_mean" + config['orfstats_mean_threshold'] + "_median" + config['orfstats_median_threshold'] + ".log"
    benchmark:
        local_path + "benchmarks/concatenate_all_tables/concatenate.benchmark_mean" + config['orfstats_mean_threshold'] + "_median" + config['orfstats_median_threshold'] + ".txt"
    params:
        outdir = local_path + "RESULTS/Bam2Reads_genome_output/"
    run:
        selected_tables = []
        for table in input.tables_list:
            if os.stat(table).st_size > 0:
                selected_tables.append(table)
        table_string = ' '.join(selected_tables)
        print(table_string)
        shell("python3 " + ribodoc_tools + "Bam2Reads_function/concatenate.py -tables " + table_string + " -outpath {params.outdir} -outname all_samples_genome" + frag_length_L + ".mean" + config['orfstats_mean_threshold'] + "_median" + config['orfstats_median_threshold'])
